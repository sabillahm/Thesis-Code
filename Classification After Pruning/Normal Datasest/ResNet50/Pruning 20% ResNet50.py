# -*- coding: utf-8 -*-
"""1 iterasi- Pruning ResNet 20% Percobaan 6 Fix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g7aM-HadiEQ84Yde8rDrALf2HOphz8Kn

# Key findings: The model size is not reduced

# Cell 1: Import Libraries and Setup Device
'''
This cell imports all the necessary libraries and sets up the device for GPU computation if available.
We are using PyTorch for deep learning, OpenCV for image processing, and Matplotlib/Seaborn for visualization.
'''
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
import cv2
from torchvision.transforms import v2
import matplotlib.pyplot as plt
import matplotlib.patheffects as path_effects
from tqdm import tqdm
import os
import numpy as np
import time
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Cek apakah kode dijalankan di Google Colab
IN_COLAB = 'google.colab' in str(get_ipython())

# Cek apakah GPU tersedia, jika tidak gunakan CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Mount Google Drive jika menggunakan Google Colab
if IN_COLAB:
    from google.colab import drive
    drive.mount('/content/drive')

"""# Cell 2: Define Data Loading and Preprocessing Functions
'''
This cell contains functions to load and preprocess images from the dataset.
The `load_data` function loads images from the specified directory and assigns labels based on the folder structure.
The `preprocess_image` function applies CLAHE and unsharp masking to enhance image quality.
The `CustomDataset` class integrates these functions and prepares the dataset for use in PyTorch.
'''
"""

def load_data(dataset_path, subset):
    images = []
    labels = []

    # Tentukan label sesuai dengan subset

    label_names = ['adenocarcinoma', 'large.cell.carcinoma', 'normal', 'squamous.cell.carcinoma']



    # Pemetaan nama label ke indeks
    label_map = {name: idx for idx, name in enumerate(label_names)}

    subset_path = os.path.join(dataset_path, subset)
    for label_name in label_names:
        image_dir = os.path.join(subset_path, label_name)
        if not os.path.exists(image_dir):
            raise FileNotFoundError(f"Directory not found: {image_dir}")
        for file_name in os.listdir(image_dir):
            if file_name.endswith('.png'):  # Cek jika file adalah gambar PNG
                image_path = os.path.join(image_dir, file_name)
                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
                if image is None:
                    print(f"Warning: Unable to load image {image_path}")
                    continue
                images.append(image)
                labels.append(label_map[label_name])
                print(f"Loaded image {image_path}")
    return images, labels

def preprocess_image(image):
    if image is None or image.size == 0:
        raise ValueError("Empty image provided for preprocessing")

    # Terapkan CLAHE untuk meningkatkan kontras
    #clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(10, 10))

    clahe_image = clahe.apply(image)

    # Terapkan Gaussian Blur untuk menghaluskan gambar
    #gaussian = cv2.GaussianBlur(image, (9, 9), 10.0)
    # Terapkan Unsharp Masking untuk meningkatkan ketajaman gambar
    #unsharp_image = cv2.addWeighted(clahe_image, 1.5, gaussian, -0.5, 0, clahe_image)
    #unsharp_image = cv2.addWeighted(gaussian, 1.5, gaussian, -0.5, 0)

    return clahe_image

class CustomDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        try:
            # Proses gambar
            image = preprocess_image(image)
        except ValueError as e:
            print(f"Error preprocessing image at index {idx}: {e}")
            plt.imshow(image, cmap='gray')
            plt.title(f"Label: {label}")
            plt.show()
            raise e
        # Terapkan transformasi jika ada
        if self.transform:
            image = self.transform(image)
        if image.dim() == 2:  # Pastikan gambar memiliki 3 dimensi
            image = image.unsqueeze(0)
        return image, label

"""# Cell 3: Load and Transform Data
'''
This cell loads the data and applies the necessary transformations.
The dataset is divided into training, validation, and test sets.
Each image is transformed into a 224x224 size tensor suitable for input to the VGG-16 model.
'''
"""

# Mengatur path dataset, disesuaikan untuk Google Colab atau Jupyter Notebook
dataset_path = '/content/drive/MyDrive/Chest-CT-Data'

# Load data
train_images, train_labels = load_data(dataset_path, 'train')
valid_images, valid_labels = load_data(dataset_path, 'valid')
test_images, test_labels = load_data(dataset_path, 'test')

print(f"Loaded {len(train_images)} training images with {len(train_labels)} labels")
print(f"Loaded {len(valid_images)} validation images with {len(valid_labels)} labels")
print(f"Loaded {len(test_images)} test images with {len(test_labels)} labels")

# Transformasi data untuk persiapan training, validation, dan testing
transform = transforms.Compose([
    transforms.ToPILImage(),             # Ubah dari array NumPy ke gambar PIL
    transforms.Resize((224, 224)),       # Ubah ukuran gambar menjadi 224x224
    #v2.RandomHorizontalFlip(),           # Randomly flip the image horizontally
   # v2.RandomRotation(15),                # Randomly rotate the image by up to 15 degrees
    transforms.ToTensor(),               # Ubah gambar menjadi Tensor and normalize to [0, 1]
])

# Membuat dataset dan data loader untuk training, validation, dan testing
train_dataset = CustomDataset(train_images, train_labels, transform=transform)
valid_dataset = CustomDataset(valid_images, valid_labels, transform=transform)
test_dataset = CustomDataset(test_images, test_labels, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # DataLoader untuk training
valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False) # DataLoader untuk validasi
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)   # DataLoader untuk testing

"""## Pruning"""

# Prune a convolutional layer based on L2 norm
def prune_conv_layer(layer, prune_ratio, in_channels=None):
    out_channels = layer.out_channels
    num_keep = max(1, int(out_channels * (1 - prune_ratio)))

    weight = layer.weight.data.cpu()
    l2_norm = torch.norm(weight.view(weight.size(0), -1), p=2, dim=1)
    top_indices = torch.argsort(l2_norm, descending=True)[:num_keep]

    mask = torch.zeros(out_channels, device=weight.device)
    mask[top_indices] = 1

    layer.weight.data = layer.weight.data[top_indices]
    if layer.bias is not None:
        layer.bias.data = layer.bias.data[top_indices]

    layer.out_channels = num_keep

    if in_channels is not None:
        layer.weight.data = layer.weight.data[:, :in_channels]
        layer.in_channels = in_channels

    return mask, num_keep

# Prune a batch normalization layer
def prune_bn_layer(layer, mask):
    layer.weight.data = layer.weight.data[mask.bool()]
    layer.bias.data = layer.bias.data[mask.bool()]
    layer.running_mean = layer.running_mean[mask.bool()]
    layer.running_var = layer.running_var[mask.bool()]
    layer.num_features = int(mask.sum().item())

# Prune bottleneck block in ResNet
def prune_bottleneck(bottleneck, prune_ratio, in_channels):
    # Prune conv1
    mask1, out1 = prune_conv_layer(bottleneck.conv1, prune_ratio, in_channels)
    prune_bn_layer(bottleneck.bn1, mask1)

    # Prune conv2
    mask2, out2 = prune_conv_layer(bottleneck.conv2, prune_ratio, out1)
    prune_bn_layer(bottleneck.bn2, mask2)

    # Prune conv3
    mask3, out3 = prune_conv_layer(bottleneck.conv3, prune_ratio, out2)
    prune_bn_layer(bottleneck.bn3, mask3)

    # Handle downsample if it exists
    if bottleneck.downsample is not None:
        mask_down, _ = prune_conv_layer(bottleneck.downsample[0], prune_ratio, in_channels)
        prune_bn_layer(bottleneck.downsample[1], mask_down)

    return out3

# Prune each layer in ResNet
def prune_resnet_layer(layer, prune_ratio, in_channels):
    for bottleneck in layer:
        out_channels = prune_bottleneck(bottleneck, prune_ratio, in_channels)
        in_channels = out_channels
    return out_channels

# Prune the entire model
def prune_model_resnet(model, prune_ratio):
    in_channels = 3  # Initial input channels for ResNet
    mask, out_channels = prune_conv_layer(model.features[0], prune_ratio, in_channels)
    prune_bn_layer(model.features[1], mask)

    # Prune ResNet layers (layer1, layer2, layer3, layer4)
    for i in range(4, 8):
        out_channels = prune_resnet_layer(model.features[i], prune_ratio, out_channels)

    # Adjust the classifier input size based on pruned features
    with torch.no_grad():
        # Create a dummy input to get the output size of the features
        dummy_input = torch.randn(1, 3, 224, 224).to(next(model.parameters()).device)
        features_output = model.features(dummy_input)
        flattened_size = features_output.view(features_output.size(0), -1).size(1)

    # Update the Linear layer in the classifier to match the pruned model
    model.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(flattened_size, 512),  # Adjust input size to match the pruned model's output
        nn.ReLU(),
        nn.Dropout(0.4),
        nn.Linear(512, model.classifier[-1].out_features)
    )

    print(f"Adjusted classifier input size to {flattened_size}")
    return model

# Load and prune the model with adjustment to the classifier
def load_and_prune_model_resnet(model_path, prune_ratio):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Resnet50()
    model.load_state_dict(torch.load(model_path, map_location=device))
    print("Original model loaded.")

    # Prune the model and adjust the classifier
    pruned_model = prune_model_resnet(model, prune_ratio).to(device)

    return pruned_model

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
def calculate_model_size(model):
    """
    Calculate the size of the model in megabytes (MB).

    Args:
    - model: The model (before or after pruning).

    Returns:
    - size_in_mb: The size of the model in MB.
    """
    param_size = sum(param.numel() * param.element_size() for param in model.parameters())
    buffer_size = sum(buffer.numel() * buffer.element_size() for buffer in model.buffers())
    size_in_bytes = param_size + buffer_size
    size_in_mb = size_in_bytes / 1024**2  # Convert bytes to megabytes (MB)
    return size_in_mb

# Fungsi untuk memverifikasi pruning model
def verify_pruned_model(model, original_model):
    def calculate_num_params(model):
        return sum(p.numel() for p in model.parameters())

    # Calculate the model sizes
    original_size = calculate_model_size(original_model)
    pruned_size = calculate_model_size(model)
    print(f"Original model size: {original_size:.2f} MB")
    print(f"Pruned model size: {pruned_size:.2f} MB")
    print(f"Size reduction: {original_size - pruned_size:.2f} MB ({(original_size - pruned_size) / original_size * 100:.2f}%)")

    original_params = calculate_num_params(original_model)
    pruned_params = calculate_num_params(model)
    print(f"Original model parameters: {original_params}")
    print(f"Pruned model parameters: {pruned_params}")

    print("\nLayer-wise dimension comparison:")
    for (orig_name, orig_module), (pruned_name, pruned_module) in zip(original_model.features.named_modules(), model.features.named_modules()):
        if isinstance(orig_module, nn.Conv2d) and isinstance(pruned_module, nn.Conv2d):
            print(f"Layer: {orig_name}")
            print(f"  - Original shape: {orig_module.weight.shape}")
            print(f"  - Pruned shape: {pruned_module.weight.shape}")

from torchvision.models import resnet50, ResNet50_Weights
import copy

# Custom ResNet50 Class
class Resnet50(nn.Module):
    def __init__(self):
        super(Resnet50, self).__init__()
        resnet_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
        self.features = nn.Sequential(*list(resnet_model.children())[:-1])
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 4)
        )

    def forward(self, input):
        x = self.features(input)
        x = self.classifier(x)
        return x

# Load model ResNet50 pretrained
model_resnet = Resnet50()
resnet_path = '/content/drive/MyDrive/best-model-resnet-clahe-0,001-8-70.pth'
model_resnet.load_state_dict(torch.load(resnet_path, map_location=device))

# Buat salinan model asli
resnet_model_copy = copy.deepcopy(model_resnet)

# Prune model dengan pruning ratio tertentu
pruning_amount = 0.2  # Contoh: prune 50% filter
pruned_model_resnet = load_and_prune_model_resnet(resnet_path, pruning_amount)

# Model sekarang sudah di-prune dan siap untuk digunakan
verify_pruned_model(pruned_model_resnet, resnet_model_copy)
torch.save(pruned_model_resnet.state_dict(), '/content/drive/MyDrive/Best-pruning-model/pruned-resnet-20.pth')

import time  # Import modul time

# Define the optimizer and loss function
optimizer = optim.SGD(pruned_model_resnet.parameters(), lr=0.001, momentum=0.8)
criterion = nn.CrossEntropyLoss()

# Function to calculate metrics (accuracy, precision, recall, F1-score)
def calculate_metrics(predictions, labels):
    preds = torch.argmax(predictions, dim=1).cpu().numpy()
    labels = labels.cpu().numpy()

    accuracy = (preds == labels).mean() * 100
    precision = precision_score(labels, preds, average='weighted', zero_division=1)
    recall = recall_score(labels, preds, average='weighted', zero_division=1)
    f1 = f1_score(labels, preds, average='weighted', zero_division=1)

    return accuracy, precision, recall, f1

# Function to train and evaluate the model
def train_and_evaluate(pruned_model, train_loader, valid_loader,  num_epochs=100):
    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []
    all_train_precisions = []
    all_train_recalls = []
    all_train_f1s = []
    all_valid_precisions = []
    all_valid_recalls = []
    all_valid_f1s = []

    best_valid_loss = float('inf')

    # Variable to accumulate total training and validation time
    total_train_validation_time = 0

    for epoch in range(1, num_epochs + 1):
        print(f'Epoch {epoch}/{num_epochs}')

        # Start timer for both training and validation
        start_epoch_time = time.time()

        # Training
        pruned_model_resnet.train()
        train_loss = 0
        all_train_preds = []
        all_train_labels = []
        for images, labels in tqdm(train_loader, desc='Training'):
            images, labels = images.to(device), labels.to(device)

            # Convert grayscale images to 3 channels
            if images.size(1) == 1:  # If single channel
                images = images.repeat(1, 3, 1, 1)  # Repeat the channel dimension 3 times

            optimizer.zero_grad()
            outputs = pruned_model_resnet(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

            all_train_preds.append(outputs)
            all_train_labels.append(labels)

        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(torch.cat(all_train_preds), torch.cat(all_train_labels))
        train_accuracies.append(train_accuracy)
        all_train_precisions.append(train_precision) # Store train_precision
        all_train_recalls.append(train_recall)       # Store train_recall
        all_train_f1s.append(train_f1)             # Store train_f1

        print(f"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1-Score: {train_f1:.4f}")

        # Validation
        pruned_model_resnet.eval()
        valid_loss = 0
        all_valid_preds = []
        all_valid_labels = []
        with torch.no_grad():
            for images, labels in tqdm(valid_loader, desc='Validation'):
                images, labels = images.to(device), labels.to(device)

                # Convert grayscale images to 3 channels
                if images.size(1) == 1:
                    images = images.repeat(1, 3, 1, 1)

                outputs = pruned_model_resnet(images)
                loss = criterion(outputs, labels)
                valid_loss += loss.item()

                all_valid_preds.append(outputs)
                all_valid_labels.append(labels)

        valid_loss /= len(valid_loader)
        valid_losses.append(valid_loss)
        valid_accuracy, valid_precision, valid_recall, valid_f1 = calculate_metrics(torch.cat(all_valid_preds), torch.cat(all_valid_labels))
        valid_accuracies.append(valid_accuracy)
        all_valid_precisions.append(valid_precision)  # Store valid_precision
        all_valid_recalls.append(valid_recall)        # Store valid_recall
        all_valid_f1s.append(valid_f1)

        print(f"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.2f}%, Precision: {valid_precision:.4f}, Recall: {valid_recall:.4f}, F1-Score: {valid_f1:.4f}")

        # Save the best model based on validation loss
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(pruned_model_resnet.state_dict(), '/content/drive/MyDrive/Best-pruning-model/best-pruned-resnet-20.pth')
            print("New best model saved")

        # End timer for training and validation, and accumulate the time
        end_epoch_time = time.time()
        epoch_time = end_epoch_time - start_epoch_time
        total_train_validation_time += epoch_time
        #print(f"Epoch Time (Training + Validation): {epoch_time:.2f} seconds")

    # Print total training and validation time
    print(f"Total Training + Validation Time: {total_train_validation_time:.2f} seconds")

    return train_losses, valid_losses, train_accuracies, valid_accuracies, all_train_precisions, all_train_recalls, all_train_f1s, all_valid_precisions, all_valid_recalls, all_valid_f1s # Return the stored metrics

# Load Model Hasil Pruning
pruned_path_resnet = '/content/drive/MyDrive/Best-pruning-model/pruned-resnet-20.pth'
pruned_model_resnet.load_state_dict(torch.load(pruned_path_resnet, map_location=device))
pruned_model_resnet = pruned_model_resnet.to(device)


# Fine-tuning model yang telah di-prune
num_epochs = 70  # Tentukan jumlah epoch untuk fine-tuning

# Training
train_losses, valid_losses, train_accuracies, valid_accuracies, train_precisions, train_recalls, train_f1s, valid_precisions, valid_recalls, valid_f1s = train_and_evaluate(pruned_model_resnet, train_loader, valid_loader, num_epochs=num_epochs)

# Simpan model setelah fine-tuning
torch.save(pruned_model_resnet.state_dict(), '/content/drive/MyDrive/Best-pruning-model/fine_tuned_pruned_resnet-20.pth')

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(valid_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(valid_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Hitung rata-rata metrik setelah semua epoch
average_train_loss = np.mean(train_losses)
average_valid_loss = np.mean(valid_losses)
average_train_accuracy = np.mean(train_accuracies)
average_valid_accuracy = np.mean(valid_accuracies)
average_train_precision = np.mean(train_precisions) # Calculate mean from stored values
average_valid_precision = np.mean(valid_precisions) # Calculate mean from stored values
average_train_recall = np.mean(train_recalls)     # Calculate mean from stored values
average_valid_recall = np.mean(valid_recalls)     # Calculate mean from stored values
average_train_f1 = np.mean(train_f1s)           # Calculate mean from stored values
average_valid_f1 = np.mean(valid_f1s)           # Calculate mean from stored values



# Display results training
print('--------------')
print(f'Training Accuracy: {average_train_accuracy:.2f}%')
print(f'Training Loss: {average_train_loss:.4f}')
print(f'Training Precision: {average_train_precision:.4f}')
print(f'Training Recall: {average_train_recall:.4f}')
print(f'Training F1-Score: {average_train_f1:.4f}')


# Display results validasi
print('--------------')
print(f'Validation Accuracy: {average_valid_accuracy:.2f}%')
print(f'Validation Loss: {average_valid_loss:.4f}')
print(f'Validation Precision: {average_valid_precision:.4f}')
print(f'Validation Recall: {average_valid_recall:.4f}')
print(f'Validation F1-Score: {average_valid_f1:.4f}')

def debug_pruned_weights(model):
    """
    Print out the number of filters and weights in each layer to ensure pruning is correctly applied.
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            print(f"Layer: {name}, Weights Shape: {module.weight.shape}")

# Periksa model setelah pruning
debug_pruned_weights(pruned_model_resnet)

# Test the model

pruned_model_resnet.load_state_dict(torch.load('/content/drive/MyDrive/Best-pruning-model/fine_tuned_pruned_resnet-20.pth'))
pruned_model_resnet.eval()

# Start timer for testing
start_test_time = time.time()

test_loss = 0
all_test_preds = []
all_test_labels = []
correct = 0
total = 0
with torch.no_grad():
  for images, labels in tqdm(test_loader, desc='Testing'):
    images, labels = images.to(device), labels.to(device)

    # Convert grayscale images to 3 channels
    if images.size(1) == 1:
      images = images.repeat(1, 3, 1, 1)

    outputs = pruned_model_resnet(images)
    loss = criterion(outputs, labels)
    test_loss += loss.item()

    all_test_preds.append(outputs)
    all_test_labels.append(labels)

    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()

test_loss /= len(test_loader)
test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(torch.cat(all_test_preds), torch.cat(all_test_labels))
print("\n---------------------------------------------------------------------------------------------\n")
print(f"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-Score: {test_f1:.4f}")

# End timer for testing and print the total time
end_test_time = time.time()
total_testing_time = end_test_time - start_test_time
print(f"Total Testing Time: {total_testing_time:.2f} seconds")

# Path dinamis untuk memuat model terbaik
#best_model_file = '/content/drive/MyDrive/Chest-CT-Data/best-model-vgg-clahe-0,001.pth' if IN_COLAB else 'best-model.pth'
# Start testing time
testing_start_time = time.time()

# Load the best model
pruned_model_resnet.load_state_dict(torch.load('/content/drive/MyDrive/Best-pruning-model/fine_tuned_pruned_resnet-20.pth'))
pruned_model_resnet.eval()

# Define class labels corresponding to the numerical labels
class_labels = [
    'Adenocarcinoma',
    'Large Cell Carcinoma',
    'Normal',
    'Squamous Cell Carcinoma'
]

predicted_probabilities = []
true_labels = []

test_loader_tqdm = tqdm(test_loader, desc="Testing")
with torch.no_grad():
    for images, labels in test_loader_tqdm:
        images, labels = images.to(device), labels.to(device)
        if images.dim() == 3:
            images = images.unsqueeze(1)
        images = images.repeat(1, 3, 1, 1)
        outputs = pruned_model_resnet(images)
        loss = criterion(outputs, labels)
        probabilities = nn.functional.softmax(outputs, dim=1)
        predicted_probabilities.append(probabilities)
        true_labels.append(labels)
        #test_loader_tqdm.set_postfix({"Batch Size": len(labels)})

# End testing time
total_testing_time = time.time() - testing_start_time
print(f"Total testing time: {total_testing_time:.2f} seconds")

# Calculate evaluation metrics
predicted_probabilities = torch.cat(predicted_probabilities)
true_labels = torch.cat(true_labels)
predicted_labels = torch.argmax(predicted_probabilities, dim=1)

accuracy = (predicted_labels == true_labels).float().mean().item()
precision = precision_score(true_labels.cpu(), predicted_labels.cpu(), average='weighted')
recall = recall_score(true_labels.cpu(), predicted_labels.cpu(), average='weighted')
f1 = f1_score(true_labels.cpu(), predicted_labels.cpu(), average='weighted')
conf_matrix = confusion_matrix(true_labels.cpu(), predicted_labels.cpu())

# Display results
print(f'Test Accuracy: {accuracy * 100:.2f}%')
print(f'Test Loss: {loss:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-Score: {f1:.4f}')
print(f'Confusion Matrix:\n{conf_matrix}')

# Plot confusion matrix using Matplotlib
fig, ax = plt.subplots(figsize=(12, 9))
cax = ax.matshow(conf_matrix, cmap='Blues')

# Menampilkan angka pada setiap sel dengan outline putih
for (i, j), val in np.ndenumerate(conf_matrix):
    text = ax.text(j, i, f'{val}', ha='center', va='center', color='black', fontsize=20, fontweight='bold')

    # Menambahkan outline putih di sekitar angka
    text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='white'),
                           path_effects.Normal()])

fig.colorbar(cax)

# Set ticks dan ticklabels
ax.set_xticks(np.arange(len(class_labels)))
ax.set_yticks(np.arange(len(class_labels)))
ax.set_xticklabels(class_labels)
ax.set_yticklabels(class_labels)

plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()


# Total time for the whole process
#total_process_time = total_training_time + total_testing_time
#print(f"Total process time (training + testing): {total_process_time:.2f} seconds")

"""perhitungan benar

![image.png](data:image/png;base64,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)
"""

# Test the model

pruned_model_resnet.load_state_dict(torch.load('/content/drive/MyDrive/Best-pruning-model/best-pruned-resnet-20.pth'))
pruned_model_resnet.eval()

test_loss = 0
all_test_preds = []
all_test_labels = []
correct = 0
total = 0
with torch.no_grad():
  for images, labels in tqdm(test_loader, desc='Testing'):
    images, labels = images.to(device), labels.to(device)

    # Convert grayscale images to 3 channels
    if images.size(1) == 1:
      images = images.repeat(1, 3, 1, 1)

    outputs = pruned_model_resnet(images)
    loss = criterion(outputs, labels)
    test_loss += loss.item()

    all_test_preds.append(outputs)
    all_test_labels.append(labels)

    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()

test_loss /= len(test_loader)
test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(torch.cat(all_test_preds), torch.cat(all_test_labels))
print("\n---------------------------------------------------------------------------------------------\n")
print(f"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-Score: {test_f1:.4f}")

